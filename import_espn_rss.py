#!/usr/bin/env python3
"""
üèÜ IMPORTADOR DE NOTICIAS ESPN RSS ‚Üí SUPABASE
==============================================
Importa noticias deportivas desde ESPN RSS Feed a la tabla 'articles' en Supabase

AUTOR: Mundial M√©xico Hub Team
FECHA: Octubre 2025
"""

import os
import sys
from datetime import datetime
import re
from typing import Dict, List, Optional
import feedparser
import requests
from supabase import create_client, Client
from slugify import slugify

# ============================================
# CONFIGURACI√ìN
# ============================================

# URLs de RSS de ESPN y otras fuentes deportivas (en orden de prioridad)
ESPN_RSS_FEEDS = [
    {
        "name": "ESPN Deportes - General",
        "url": "https://www.espn.com/espn/rss/news",
        "category": "Deportes"
    },
    {
        "name": "ESPN Soccer/F√∫tbol",
        "url": "https://www.espn.com/espn/rss/soccer/news",
        "category": "F√∫tbol"
    },
    {
        "name": "Marca - F√∫tbol",
        "url": "https://e00-marca.uecdn.es/rss/futbol/primera.xml",
        "category": "F√∫tbol"
    },
    {
        "name": "AS - F√∫tbol",
        "url": "https://as.com/rss/futbol/portada.xml",
        "category": "F√∫tbol"
    },
    {
        "name": "Goal.com - Internacional",
        "url": "https://www.goal.com/feeds/es/news",
        "category": "F√∫tbol"
    }
]

# Configuraci√≥n de Supabase
SUPABASE_URL = "https://ksiiidnvtktlowlhtebs.supabase.co"
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# ============================================
# FUNCIONES AUXILIARES
# ============================================

def clean_html(text: str) -> str:
    """Limpia HTML y caracteres extra√±os del texto"""
    if not text:
        return ""
    
    # Eliminar tags HTML
    text = re.sub(r'<[^>]+>', '', text)
    
    # Decodificar entidades HTML
    text = text.replace('&nbsp;', ' ')
    text = text.replace('&amp;', '&')
    text = text.replace('&lt;', '<')
    text = text.replace('&gt;', '>')
    text = text.replace('&quot;', '"')
    text = text.replace('&#39;', "'")
    
    # Limpiar espacios m√∫ltiples
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text


def extract_excerpt(content: str, max_length: int = 200) -> str:
    """Extrae un excerpt limpio del contenido"""
    cleaned = clean_html(content)
    
    if len(cleaned) <= max_length:
        return cleaned
    
    # Cortar en el √∫ltimo punto antes del l√≠mite
    truncated = cleaned[:max_length]
    last_period = truncated.rfind('.')
    
    if last_period > max_length * 0.5:  # Si hay un punto razonable
        return truncated[:last_period + 1]
    
    return truncated + "..."


def extract_image_from_entry(entry: any, content: str = "") -> Optional[str]:
    """
    Extrae URL de imagen de un entry de RSS
    Busca en m√∫ltiples fuentes: media:content, enclosure, links, content HTML
    """
    
    # 1. Buscar en media:content (com√∫n en RSS de noticias)
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            if 'url' in media:
                url = media['url']
                # Verificar que sea una imagen
                if any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    return url
    
    # 2. Buscar en media:thumbnail
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        for thumb in entry.media_thumbnail:
            if 'url' in thumb:
                return thumb['url']
    
    # 3. Buscar en enclosures
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enclosure in entry.enclosures:
            if 'type' in enclosure and 'image' in enclosure['type']:
                if 'href' in enclosure:
                    return enclosure['href']
    
    # 4. Buscar en links
    if hasattr(entry, 'links'):
        for link in entry.links:
            if link.get('type', '').startswith('image/'):
                if 'href' in link:
                    return link['href']
    
    # 5. Buscar en el contenido HTML
    if content:
        # Buscar tags <img>
        img_match = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', content)
        if img_match:
            return img_match.group(1)
        
        # Buscar URLs de im√°genes directas
        url_match = re.search(r'https?://[^\s<>"]+(?:jpg|jpeg|png|gif|webp)', content, re.IGNORECASE)
        if url_match:
            return url_match.group(0)
    
    return None


def article_exists(supabase: Client, slug: str) -> bool:
    """Verifica si un art√≠culo ya existe por slug"""
    try:
        result = supabase.table("articles").select("id").eq("slug", slug).execute()
        return len(result.data) > 0
    except Exception:
        return False


def parse_category_from_link(link: str, default_category: str) -> str:
    """Intenta extraer categor√≠a de la URL"""
    if 'soccer' in link or 'football' in link or 'futbol' in link:
        return "F√∫tbol"
    elif 'nfl' in link:
        return "NFL"
    elif 'nba' in link:
        return "NBA"
    elif 'mlb' in link:
        return "MLB"
    elif 'boxing' in link:
        return "Boxeo"
    
    return default_category


def determine_tags(title: str, content: str, link: str) -> List[str]:
    """Determina tags relevantes basados en el contenido"""
    tags = []
    
    # Tags base para Mundial 2026
    world_cup_keywords = ['mundial', 'world cup', '2026', 'fifa', 'mexico', 'm√©xico']
    text_to_search = (title + ' ' + content + ' ' + link).lower()
    
    if any(keyword in text_to_search for keyword in world_cup_keywords):
        tags.append("Mundial 2026")
    
    # Categor√≠as deportivas
    if any(word in text_to_search for word in ['soccer', 'football', 'futbol', 'f√∫tbol']):
        tags.append("F√∫tbol")
    
    if any(word in text_to_search for word in ['mexico', 'm√©xico', 'mexican']):
        tags.append("M√©xico")
    
    # Siempre incluir tag de ESPN
    tags.append("ESPN")
    
    return list(set(tags))  # Eliminar duplicados


# ============================================
# FUNCI√ìN PRINCIPAL DE IMPORTACI√ìN
# ============================================

def import_from_espn_rss(supabase: Client, rss_config: Dict) -> Dict[str, int]:
    """
    Importa noticias desde un feed RSS de ESPN
    
    Returns:
        Dict con estad√≠sticas: {total, imported, skipped, errors}
    """
    print(f"\n{'='*60}")
    print(f"üì° Importando desde: {rss_config['name']}")
    print(f"üîó URL: {rss_config['url']}")
    print(f"{'='*60}\n")
    
    # Headers para evitar bloqueos
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/rss+xml, application/xml, text/xml, */*'
    }
    
    try:
        # Descargar RSS con requests + headers
        response = requests.get(rss_config['url'], headers=headers, timeout=15)
        response.raise_for_status()
        
        # Parsear con feedparser
        feed = feedparser.parse(response.content)
        
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error al descargar RSS: {str(e)}")
        return {"total": 0, "imported": 0, "skipped": 0, "errors": 1}
    
    if not feed.entries:
        print(f"‚ùå No se encontraron noticias en el feed")
        return {"total": 0, "imported": 0, "skipped": 0, "errors": 0}
    
    stats = {
        "total": len(feed.entries),
        "imported": 0,
        "skipped": 0,
        "errors": 0
    }
    
    print(f"üì∞ Encontradas {stats['total']} noticias\n")
    
    for entry in feed.entries:
        try:
            # Generar slug √∫nico
            slug = slugify(entry.title)
            
            # Verificar si ya existe
            if article_exists(supabase, slug):
                print(f"‚è≠Ô∏è  Ya existe: {entry.title[:60]}...")
                stats["skipped"] += 1
                continue
            
            # Extraer datos
            title = entry.title
            link = entry.link
            content = entry.get('summary', entry.get('description', ''))
            excerpt = extract_excerpt(content)
            cover_image = extract_image_from_entry(entry, content)
            category = parse_category_from_link(link, rss_config['category'])
            tags = determine_tags(title, content, link)
            
            # Determinar autor basado en la fuente
            author = entry.get('author', None)
            if not author:
                # Usar solo el nombre de la fuente (sin la parte despu√©s del -)
                author = rss_config['name'].split(' - ')[0]
            
            # Fecha de publicaci√≥n
            published_at = None
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                try:
                    published_at = datetime(*entry.published_parsed[:6]).isoformat()
                except:
                    published_at = datetime.now().isoformat()
            else:
                published_at = datetime.now().isoformat()
            
            # Preparar datos para inserci√≥n
            article_data = {
                "title": title[:200],  # L√≠mite de la DB
                "slug": slug[:200],
                "content": clean_html(content),
                "excerpt": excerpt,
                "cover_image_url": cover_image,
                "author": author,  # ‚Üê Cambio aqu√≠
                "category": category,
                "tags": tags,
                "published_at": published_at,
                "is_featured": False,
                "views": 0
            }
            
            # Insertar en Supabase
            response = supabase.table("articles").insert(article_data).execute()
            
            if response.data:
                img_status = "üñºÔ∏è" if cover_image else "üìÑ"
                print(f"‚úÖ {img_status} Importado: {title[:65]}...")
                stats["imported"] += 1
            else:
                print(f"‚ö†Ô∏è  Error al importar: {title[:50]}")
                stats["errors"] += 1
                
        except Exception as e:
            print(f"‚ùå Error procesando entrada: {str(e)}")
            stats["errors"] += 1
            continue
    
    return stats


# ============================================
# MAIN
# ============================================

def main():
    """Funci√≥n principal"""
    
    print("\n" + "="*60)
    print("üèÜ IMPORTADOR ESPN RSS ‚Üí SUPABASE")
    print("="*60)
    
    # Verificar SUPABASE_KEY
    if not SUPABASE_KEY:
        print("\n‚ùå ERROR: Variable de entorno SUPABASE_KEY no configurada")
        print("\nüí° Soluci√≥n:")
        print("   export SUPABASE_KEY='tu_service_role_key'")
        print("\nüìñ Consigue tu key en:")
        print("   https://supabase.com/dashboard/project/ksiiidnvtktlowlhtebs/settings/api")
        sys.exit(1)
    
    # Inicializar cliente Supabase
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("‚úÖ Conectado a Supabase")
    except Exception as e:
        print(f"‚ùå Error conectando a Supabase: {str(e)}")
        sys.exit(1)
    
    # Estad√≠sticas globales
    total_stats = {
        "total": 0,
        "imported": 0,
        "skipped": 0,
        "errors": 0
    }
    
    # Procesar cada feed RSS
    for rss_config in ESPN_RSS_FEEDS:
        feed_stats = import_from_espn_rss(supabase, rss_config)
        
        # Acumular estad√≠sticas
        for key in total_stats:
            total_stats[key] += feed_stats[key]
    
    # Resumen final
    print("\n" + "="*60)
    print("üìä RESUMEN FINAL")
    print("="*60)
    print(f"üì∞ Total de noticias procesadas: {total_stats['total']}")
    print(f"‚úÖ Importadas exitosamente: {total_stats['imported']}")
    print(f"‚è≠Ô∏è  Saltadas (ya exist√≠an): {total_stats['skipped']}")
    print(f"‚ùå Errores: {total_stats['errors']}")
    print("="*60)
    
    if total_stats['imported'] > 0:
        print("\nüéâ ¬°Importaci√≥n completada exitosamente!")
        print("\nüí° Puedes ver las noticias en:")
        print("   https://lovable.dev/projects/130d2522-f5c7-46cf-8735-0f3265474abc")
    elif total_stats['skipped'] > 0:
        print("\n‚úÖ Todas las noticias ya estaban importadas")
    else:
        print("\n‚ö†Ô∏è  No se importaron noticias nuevas")


if __name__ == "__main__":
    main()

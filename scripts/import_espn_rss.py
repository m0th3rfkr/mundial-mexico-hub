#!/usr/bin/env python3
"""
RSS Importer MEJORADO v2.0
- Maneja redirects correctamente
- User-Agent mejorado
- M√∫ltiples fuentes RSS
- Extracci√≥n robusta de im√°genes
"""

import feedparser
import requests
from supabase import create_client
import os
from datetime import datetime
import time
from urllib.parse import urlparse
import re

# ============================================
# CONFIGURACI√ìN
# ============================================

SUPABASE_URL = "https://ksiiidnvtktlowlhtebs.supabase.co"
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")

# RSS Feeds actualizados y VERIFICADOS con im√°genes
RSS_FEEDS = {
    "FourFourTwo": "https://www.fourfourtwo.com/feeds/all",
    "The Athletic": "https://theathletic.com/feed/",
    "90min": "https://www.90min.com/posts.rss",
    "BBC Sport Football": "https://feeds.bbci.co.uk/sport/football/rss.xml",
    "Sky Sports Football": "https://www.skysports.com/rss/12040",
}

# Headers mejorados para evitar bloqueos
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
    'Accept-Language': 'es-MX,es;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

# ============================================
# FUNCIONES AUXILIARES
# ============================================

def extract_image_from_entry(entry):
    """
    Extrae imagen de m√∫ltiples formatos RSS con prioridad
    """
    # Prioridad 1: media:content (ESPN, TUDN)
    if hasattr(entry, 'media_content') and entry.media_content:
        for media in entry.media_content:
            url = media.get('url', '')
            if url and any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                return url
    
    # Prioridad 2: media:thumbnail
    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
        return entry.media_thumbnail[0].get('url')
    
    # Prioridad 3: enclosures
    if hasattr(entry, 'enclosures') and entry.enclosures:
        for enc in entry.enclosures:
            if 'image' in enc.get('type', ''):
                return enc.get('href')
    
    # Prioridad 4: Buscar en summary/description
    for field in ['summary', 'description', 'content']:
        if hasattr(entry, field):
            text = getattr(entry, field)
            if isinstance(text, list):
                text = text[0].get('value', '') if text else ''
            
            # Buscar URLs de im√°genes
            img_patterns = [
                r'<img[^>]+src=["\']([^"\']+)["\']',
                r'https?://[^\s]+\.(?:jpg|jpeg|png|webp)',
            ]
            
            for pattern in img_patterns:
                matches = re.findall(pattern, str(text), re.IGNORECASE)
                if matches:
                    return matches[0]
    
    return None


def create_slug(title):
    """Crear slug SEO-friendly"""
    slug = title.lower()
    # Reemplazar acentos
    replacements = {
        '√°': 'a', '√©': 'e', '√≠': 'i', '√≥': 'o', '√∫': 'u',
        '√±': 'n', '√º': 'u'
    }
    for old, new in replacements.items():
        slug = slug.replace(old, new)
    
    # Solo alfanum√©ricos y guiones
    slug = re.sub(r'[^a-z0-9]+', '-', slug)
    slug = slug.strip('-')
    return slug[:100]


def fetch_rss(url, timeout=15):
    """
    Descarga RSS con manejo robusto de errores
    """
    try:
        # Configurar sesi√≥n con redirects
        session = requests.Session()
        session.max_redirects = 10
        
        response = session.get(
            url,
            headers=HEADERS,
            timeout=timeout,
            allow_redirects=True,
            verify=True
        )
        response.raise_for_status()
        
        # Parsear con feedparser
        feed = feedparser.parse(response.content)
        
        if feed.bozo and not feed.entries:
            print(f"   ‚ö†Ô∏è  Feed malformado o vac√≠o")
            return None
        
        return feed
        
    except requests.exceptions.TooManyRedirects:
        print(f"   ‚ùå Demasiados redirects")
        return None
    except requests.exceptions.Timeout:
        print(f"   ‚ùå Timeout despu√©s de {timeout}s")
        return None
    except requests.exceptions.RequestException as e:
        print(f"   ‚ùå Error de conexi√≥n: {str(e)[:50]}")
        return None
    except Exception as e:
        print(f"   ‚ùå Error inesperado: {str(e)[:50]}")
        return None


def import_news_from_rss(supabase, rss_url, source_name):
    """
    Importa noticias desde un RSS feed
    """
    print(f"\n{'='*70}")
    print(f"üì∞ {source_name}")
    print(f"{'='*70}")
    print(f"üì° {rss_url}")
    
    # Descargar RSS
    feed = fetch_rss(rss_url)
    if not feed or not feed.entries:
        return 0
    
    print(f"‚úÖ {len(feed.entries)} entradas encontradas")
    
    imported_count = 0
    skipped_count = 0
    error_count = 0
    
    for entry in feed.entries[:20]:  # Limitar a 20 por fuente
        try:
            title = entry.get('title', 'Sin t√≠tulo').strip()
            if not title or len(title) < 10:
                continue
            
            slug = create_slug(title)
            
            # Verificar duplicado
            existing = supabase.table('articles')\
                .select('id')\
                .eq('slug', slug)\
                .execute()
            
            if existing.data:
                skipped_count += 1
                continue
            
            # Extraer datos
            excerpt = entry.get('summary', '')[:500] if entry.get('summary') else None
            content_raw = entry.get('content', [{}])[0].get('value', entry.get('summary', ''))
            content = re.sub(r'<[^>]+>', '', content_raw)[:5000]  # Limpiar HTML
            
            # Extraer imagen
            cover_image_url = extract_image_from_entry(entry)
            
            # Fecha de publicaci√≥n
            pub_date = entry.get('published', entry.get('updated', ''))
            published_at = None
            if pub_date:
                try:
                    from dateutil import parser
                    published_at = parser.parse(pub_date).isoformat()
                except:
                    published_at = datetime.now().isoformat()
            else:
                published_at = datetime.now().isoformat()
            
            # Preparar art√≠culo
            article_data = {
                'title': title,
                'slug': slug,
                'content': content,
                'excerpt': excerpt,
                'cover_image_url': cover_image_url,
                'author': source_name,
                'category': 'Deportes',
                'tags': ['Mundial 2026', 'F√∫tbol'],
                'published_at': published_at,
                'is_featured': False,
                'views': 0
            }
            
            # Insertar en Supabase
            supabase.table('articles').insert(article_data).execute()
            
            # Mostrar resultado
            icon = "üñºÔ∏è " if cover_image_url else "üìÑ"
            print(f"  ‚úÖ {icon} {title[:65]}...")
            
            imported_count += 1
            
        except Exception as e:
            error_count += 1
            print(f"  ‚ùå Error: {str(e)[:50]}")
            continue
    
    # Resumen
    print(f"\nüìä Resumen {source_name}:")
    print(f"   ‚úÖ Importadas: {imported_count}")
    print(f"   ‚è≠Ô∏è  Omitidas: {skipped_count}")
    if error_count > 0:
        print(f"   ‚ùå Errores: {error_count}")
    
    return imported_count


# ============================================
# MAIN
# ============================================

def main():
    """Funci√≥n principal"""
    
    print("="*70)
    print("üöÄ RSS IMPORTER v2.0 - MEJORADO")
    print("="*70)
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # Validar Service Role Key
    if not SUPABASE_KEY:
        print("\n‚ùå ERROR: SUPABASE_KEY no configurada")
        print("\nüí° Ejecuta:")
        print("   export SUPABASE_KEY='tu_service_role_key'")
        return
    
    # Conectar a Supabase
    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("‚úÖ Conectado a Supabase")
    except Exception as e:
        print(f"‚ùå Error conectando: {e}")
        return
    
    # Importar desde cada fuente
    total_imported = 0
    successful_sources = 0
    
    for source_name, rss_url in RSS_FEEDS.items():
        imported = import_news_from_rss(supabase, rss_url, source_name)
        if imported > 0:
            successful_sources += 1
        total_imported += imported
        time.sleep(2)  # Pausa entre fuentes
    
    # Resumen final
    print("\n" + "="*70)
    print("üéâ RESUMEN FINAL")
    print("="*70)
    print(f"üìä Total importado: {total_imported} noticias")
    print(f"üìö Fuentes exitosas: {successful_sources}/{len(RSS_FEEDS)}")
    print("="*70)
    
    if total_imported > 0:
        print("\n‚úÖ ¬°Listo! Refresca /news para ver las nuevas noticias")
    else:
        print("\n‚ö†Ô∏è  No se importaron noticias nuevas")
        print("üí° Verifica las URLs de RSS o intenta m√°s tarde")


if __name__ == "__main__":
    main()

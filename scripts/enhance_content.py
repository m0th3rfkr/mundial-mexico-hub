"""
Script para enriquecer contenido de art√≠culos
Extrae el contenido completo de los art√≠culos desde sus URLs originales
"""

import requests
from bs4 import BeautifulSoup
from supabase import create_client
import os
import time
import re
from datetime import datetime

# Configuraci√≥n
SUPABASE_URL = "https://ksiiidnvtktlowlhtebs.supabase.co"
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "")

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
}

def extract_article_content(url):
    """
    Extrae el contenido completo de un art√≠culo desde su URL
    """
    try:
        print(f"   üì• Descargando: {url[:60]}...")
        response = requests.get(url, headers=HEADERS, timeout=10, allow_redirects=True)
        
        if response.status_code != 200:
            print(f"   ‚ùå Error {response.status_code}")
            return None
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Estrategias de extracci√≥n por fuente
        content_paragraphs = []
        
        # BBC Sport
        if 'bbc.co.uk' in url or 'bbc.com' in url:
            article = soup.find('article') or soup.find('div', {'data-component': 'text-block'})
            if article:
                content_paragraphs = article.find_all('p')
        
        # 90min
        elif '90min.com' in url:
            article = soup.find('div', class_='article-body') or soup.find('div', class_='entry-content')
            if article:
                content_paragraphs = article.find_all('p')
        
        # Sky Sports
        elif 'skysports.com' in url:
            # Intentar m√∫ltiples selectores
            article = (
                soup.find('div', class_='article__body') or 
                soup.find('div', class_='sdc-article-body') or
                soup.find('div', {'data-role': 'body'}) or
                soup.find('article')
            )
            if article:
                # Buscar todos los p√°rrafos, excluyendo los de widgets
                content_paragraphs = [
                    p for p in article.find_all('p') 
                    if not p.find_parent('aside') and not p.find_parent('div', class_='widget')
                ]
        
        # Estrategia gen√©rica
        else:
            # Intentar encontrar el art√≠culo principal
            article = (
                soup.find('article') or 
                soup.find('div', class_=re.compile(r'article|content|post|entry', re.I)) or
                soup.find('main')
            )
            if article:
                content_paragraphs = article.find_all('p')
        
        # Extraer texto de los p√°rrafos
        if content_paragraphs:
            text = '\n\n'.join([p.get_text().strip() for p in content_paragraphs if p.get_text().strip()])
            
            # Limpiar
            text = re.sub(r'\s+', ' ', text)  # Multiple spaces
            text = re.sub(r'\n\s*\n', '\n\n', text)  # Multiple newlines
            
            # Limitar a 15000 chars
            if len(text) > 15000:
                text = text[:15000] + "..."
            
            if len(text) > 200:  # Solo si es contenido sustancial
                print(f"   ‚úÖ Extra√≠do: {len(text)} caracteres")
                return text
        
        print(f"   ‚ö†Ô∏è  No se pudo extraer contenido")
        return None
        
    except Exception as e:
        print(f"   ‚ùå Error: {str(e)}")
        return None

def enhance_articles(limit=10):
    """
    Mejora el contenido de art√≠culos que tienen poco texto
    """
    print("="*70)
    print("üîß ENRIQUECEDOR DE CONTENIDO")
    print("="*70)
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*70)
    
    # Conectar a Supabase
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    print("‚úÖ Conectado a Supabase\n")
    
    # Obtener art√≠culos con poco contenido
    print(f"üîç Buscando art√≠culos con contenido corto...\n")
    
    result = supabase.table('articles').select('id, title, source_url, content, author').order('created_at', desc=True).limit(limit).execute()
    
    articles = result.data
    
    if not articles:
        print("‚ö†Ô∏è  No se encontraron art√≠culos")
        return
    
    print(f"üìä Encontrados: {len(articles)} art√≠culos\n")
    
    updated_count = 0
    skipped_count = 0
    error_count = 0
    
    for i, article in enumerate(articles, 1):
        # Saltar si no tiene URL
        if not article.get('source_url'):
            continue
            
        print(f"[{i}/{len(articles)}] üì∞ {article['title'][:60]}...")
        print(f"   üìè Contenido actual: {len(article['content'])} chars")
        
        # Si ya tiene buen contenido, omitir
        if len(article['content']) > 500:
            print(f"   ‚è≠Ô∏è  Ya tiene buen contenido\n")
            skipped_count += 1
            continue
        
        # Extraer contenido completo
        full_content = extract_article_content(article['source_url'])
        
        if full_content and len(full_content) > len(article['content']):
            # Actualizar en Supabase
            try:
                supabase.table('articles').update({'content': full_content}).eq('id', article['id']).execute()
                
                print(f"   ‚úÖ Actualizado: {len(article['content'])} ‚Üí {len(full_content)} chars\n")
                updated_count += 1
                
            except Exception as e:
                print(f"   ‚ùå Error al actualizar: {str(e)}\n")
                error_count += 1
        else:
            print(f"   ‚ö†Ô∏è  No se pudo mejorar\n")
            error_count += 1
        
        # Rate limiting
        time.sleep(1)
    
    # Resumen
    print("="*70)
    print("üéâ RESUMEN FINAL")
    print("="*70)
    print(f"‚úÖ Actualizados: {updated_count}")
    print(f"‚è≠Ô∏è  Omitidos: {skipped_count}")
    print(f"‚ùå Errores: {error_count}")
    print("="*70)

if __name__ == "__main__":
    # Procesar √∫ltimos 20 art√≠culos
    enhance_articles(limit=20)
